{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading done\n",
      "accuracy:0.9187519308001235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0     0.9911    0.9875    0.9893       562\n",
      "         2.0     0.9563    0.9850    0.9704       400\n",
      "         3.0     0.9840    0.9608    0.9723       383\n",
      "         4.0     0.8870    0.8416    0.8637       606\n",
      "         5.0     0.8307    0.8826    0.8558       528\n",
      "         6.0     0.9934    0.9983    0.9959       605\n",
      "         7.0     0.6765    0.9200    0.7797        25\n",
      "         8.0     1.0000    0.2000    0.3333        15\n",
      "         9.0     0.7083    0.4359    0.5397        39\n",
      "        10.0     0.1429    0.1111    0.1250        18\n",
      "        11.0     0.4118    0.6000    0.4884        35\n",
      "        12.0     0.5238    0.5238    0.5238        21\n",
      "\n",
      "    accuracy                         0.9188      3237\n",
      "   macro avg     0.7588    0.7039    0.7031      3237\n",
      "weighted avg     0.9209    0.9188    0.9176      3237\n",
      "\n",
      "('Unnamed: 0', 0.003145063539936353)\n",
      "('TAx_mean', 0.01631386191172002)\n",
      "('TAy_mean', 0.013979068366422438)\n",
      "('TAz_mean', 0.004657128149731)\n",
      "('TAtotal_mean', 0.007952202107090535)\n",
      "('TAx_std', 0.011816899688620581)\n",
      "('TAy_std', 0.00337228763887227)\n",
      "('TAz_std', 0.001921752592326165)\n",
      "('TAtotal_std', 0.018255593178376092)\n",
      "('TAx_median', 0.015406228170857088)\n",
      "('TAy_median', 0.015008214473556155)\n",
      "('TAz_median', 0.003653424775430454)\n",
      "('TAtotal_median', 0.007757824366691289)\n",
      "('TAx_max', 0.019435727930384162)\n",
      "('TAy_max', 0.009013459101412847)\n",
      "('TAz_max', 0.0037389910588475094)\n",
      "('TAtotal_max', 0.022653366862530874)\n",
      "('TAx_min', 0.020470985295013632)\n",
      "('TAy_min', 0.01563010864552978)\n",
      "('TAz_min', 0.0037378975930878523)\n",
      "('TAtotal_min', 0.009981402691216194)\n",
      "('TAx_range', 0.01225084599688598)\n",
      "('TAy_range', 0.002508360407769135)\n",
      "('TAz_range', 0.0012303256502682247)\n",
      "('TAtotal_range', 0.014624603242862688)\n",
      "('TAx_iqr', 0.00887931218348683)\n",
      "('TAy_iqr', 0.002874548568188708)\n",
      "('TAz_iqr', 0.0014331900597935798)\n",
      "('TAtotal_iqr', 0.013514421750958528)\n",
      "('TAx_skew', 0.001174199645377338)\n",
      "('TAy_skew', 0.0007966168409060582)\n",
      "('TAz_skew', 0.0008892882538707175)\n",
      "('TAtotal_skew', 0.0012625767908737578)\n",
      "('TAx_entropy', 0.011540613626030843)\n",
      "('TAy_entropy', 0.002736266277163408)\n",
      "('TAz_entropy', 0.0020759747532185986)\n",
      "('TAtotal_entropy', 0.012552463064012755)\n",
      "('TA_sma', 0.022269279562121855)\n",
      "('TAfreq_feature0', 0.00020385216993959633)\n",
      "('TAfreq_feature1', 0.00021645728506756152)\n",
      "('TAfreq_feature2', 0.00021153521084273558)\n",
      "('TAfreq_feature3', 0.00023080289387176153)\n",
      "('TAfreq_feature4', 0.00019513302406451216)\n",
      "('TAfreq_feature5', 0.00010720173480949064)\n",
      "('TAfreq_feature6', 0.00021328607502179328)\n",
      "('TAfreq_feature7', 0.00021872895954164293)\n",
      "('TAfreq_feature8', 0.00022206142816588113)\n",
      "('TAfreq_feature9', 0.0002544198374145397)\n",
      "('TAfreq_feature10', 0.0001912749605374556)\n",
      "('TAfreq_feature11', 0.00010975344970185643)\n",
      "('TAfreq_feature12', 0.0007796929407861781)\n",
      "('TAfreq_feature13', 0.016956226179915147)\n",
      "('BAx_mean', 0.0005896297395410171)\n",
      "('BAy_mean', 0.0005287672369708256)\n",
      "('BAz_mean', 0.0005215552149469136)\n",
      "('BAtotal_mean', 0.013552450705093434)\n",
      "('BAx_std', 0.0114463395076493)\n",
      "('BAy_std', 0.0036280946735894183)\n",
      "('BAz_std', 0.0019166047913921369)\n",
      "('BAtotal_std', 0.005426132874082474)\n",
      "('BAx_median', 0.0006172580719667474)\n",
      "('BAy_median', 0.0008991398713545049)\n",
      "('BAz_median', 0.0008672758168439049)\n",
      "('BAtotal_median', 0.015583782430563374)\n",
      "('BAx_max', 0.011541371753546855)\n",
      "('BAy_max', 0.0016756451726061745)\n",
      "('BAz_max', 0.0028068460878365588)\n",
      "('BAtotal_max', 0.011331288399503352)\n",
      "('BAx_min', 0.006987950720339522)\n",
      "('BAy_min', 0.002090757615685727)\n",
      "('BAz_min', 0.0013738440396897321)\n",
      "('BAtotal_min', 0.009280491666642254)\n",
      "('BAx_range', 0.00988991761225278)\n",
      "('BAy_range', 0.0028597611002378425)\n",
      "('BAz_range', 0.0012891443286782295)\n",
      "('BAtotal_range', 0.0044126650067211875)\n",
      "('BAx_iqr', 0.01582965748759682)\n",
      "('BAy_iqr', 0.0038669488988798493)\n",
      "('BAz_iqr', 0.003492901594853741)\n",
      "('BAtotal_iqr', 0.00552775292462558)\n",
      "('BAx_skew', 0.0009655921171782709)\n",
      "('BAy_skew', 0.0007904891810476267)\n",
      "('BAz_skew', 0.0008391024819944011)\n",
      "('BAtotal_skew', 0.0006882859039694071)\n",
      "('BAx_entropy', 0.0005869564702376199)\n",
      "('BAy_entropy', 0.0006356272358741072)\n",
      "('BAz_entropy', 0.0005949320830362052)\n",
      "('BAtotal_entropy', 0.011011106723213412)\n",
      "('BA_sma', 0.017162473473641023)\n",
      "('BAfreq_feature0', 0.00028061564741261636)\n",
      "('BAfreq_feature1', 0.0002601996256624375)\n",
      "('BAfreq_feature2', 0.00023980842731992867)\n",
      "('BAfreq_feature3', 0.00034297956942034604)\n",
      "('BAfreq_feature4', 0.0002762555447704896)\n",
      "('BAfreq_feature5', 0.0002230830771045223)\n",
      "('BAfreq_feature6', 0.0002771984622647311)\n",
      "('BAfreq_feature7', 0.0002507407849404504)\n",
      "('BAfreq_feature8', 0.00024391680713809754)\n",
      "('BAfreq_feature9', 0.0003099766203517844)\n",
      "('BAfreq_feature10', 0.00022849054420475518)\n",
      "('BAfreq_feature11', 0.0002340971438145945)\n",
      "('BAfreq_feature12', 0.0007887485479276472)\n",
      "('BAfreq_feature13', 0.004518380306175747)\n",
      "('GAx_mean', 0.020031986619635594)\n",
      "('GAy_mean', 0.015711726739320583)\n",
      "('GAz_mean', 0.003599363835017878)\n",
      "('GAtotal_mean', 0.0068670017361197825)\n",
      "('GAx_std', 0.0015110139985754572)\n",
      "('GAy_std', 0.0026974979178461126)\n",
      "('GAz_std', 0.0016873798870474413)\n",
      "('GAtotal_std', 0.0006010010010656459)\n",
      "('GAx_median', 0.013683466135422363)\n",
      "('GAy_median', 0.016780965320600284)\n",
      "('GAz_median', 0.004106892649006372)\n",
      "('GAtotal_median', 0.008497804601000417)\n",
      "('GAx_max', 0.02304345023048033)\n",
      "('GAy_max', 0.012428328422269758)\n",
      "('GAz_max', 0.004304682798515624)\n",
      "('GAtotal_max', 0.005522072453822146)\n",
      "('GAx_min', 0.013128538170444475)\n",
      "('GAy_min', 0.015045120551476813)\n",
      "('GAz_min', 0.0038894424894984772)\n",
      "('GAtotal_min', 0.005561036146679301)\n",
      "('GAx_range', 0.001691763494347246)\n",
      "('GAy_range', 0.002724210741149043)\n",
      "('GAz_range', 0.0016920012039313831)\n",
      "('GAtotal_range', 0.0007143059702773337)\n",
      "('GAx_iqr', 0.0017875268111631182)\n",
      "('GAy_iqr', 0.0024291895516770044)\n",
      "('GAz_iqr', 0.001978737590493984)\n",
      "('GAtotal_iqr', 0.0007446130124793788)\n",
      "('GAx_skew', 0.00065947365066771)\n",
      "('GAy_skew', 0.0005858827219085701)\n",
      "('GAz_skew', 0.000525679680722735)\n",
      "('GAtotal_skew', 0.0005297666451631777)\n",
      "('GAx_entropy', 0.002280905562132846)\n",
      "('GAy_entropy', 0.0017773456513053868)\n",
      "('GAz_entropy', 0.0015287575368060926)\n",
      "('GAtotal_entropy', 0.0007803665849206071)\n",
      "('GA_sma', 0.019373747097762527)\n",
      "('GAfreq_feature0', 2.570497242121298e-05)\n",
      "('GAfreq_feature1', 4.302393100214277e-05)\n",
      "('GAfreq_feature2', 2.2318714208584623e-05)\n",
      "('GAfreq_feature3', 5.6970497846724395e-05)\n",
      "('GAfreq_feature4', 5.217619739399408e-05)\n",
      "('GAfreq_feature5', 3.6420647599102294e-05)\n",
      "('GAfreq_feature6', 2.8119468990489967e-05)\n",
      "('GAfreq_feature7', 5.110270566491839e-05)\n",
      "('GAfreq_feature8', 2.9775274383936933e-05)\n",
      "('GAfreq_feature9', 4.764817872436422e-05)\n",
      "('GAfreq_feature10', 5.111486712527668e-05)\n",
      "('GAfreq_feature11', 3.6798766170548455e-05)\n",
      "('GAfreq_feature12', 0.0006051474067794913)\n",
      "('GAfreq_feature13', 0.0005581602160786983)\n",
      "('BJx_mean', 0.0005727223218112412)\n",
      "('BJy_mean', 0.0005294593167331607)\n",
      "('BJz_mean', 0.0005621351004508806)\n",
      "('BJtotal_mean', 0.008925985710993138)\n",
      "('BJx_std', 0.010326143020039993)\n",
      "('BJy_std', 0.0021464668233276314)\n",
      "('BJz_std', 0.0019459060470652931)\n",
      "('BJtotal_std', 0.015268787009860008)\n",
      "('BJx_median', 0.0006147361445954246)\n",
      "('BJy_median', 0.0009604980866994719)\n",
      "('BJz_median', 0.0006305627951565264)\n",
      "('BJtotal_median', 0.012725777923482803)\n",
      "('BJx_max', 0.00503849597392599)\n",
      "('BJy_max', 0.0015005916257085628)\n",
      "('BJz_max', 0.0012065874590218982)\n",
      "('BJtotal_max', 0.0036498198888161256)\n",
      "('BJx_min', 0.006160939095456689)\n",
      "('BJy_min', 0.0016434948774605817)\n",
      "('BJz_min', 0.0009985244451065005)\n",
      "('BJtotal_min', 0.01766380324087675)\n",
      "('BJx_range', 0.006700369619622791)\n",
      "('BJy_range', 0.0023905742911615705)\n",
      "('BJz_range', 0.0014137449426432644)\n",
      "('BJtotal_range', 0.008275980700833841)\n",
      "('BJx_iqr', 0.014161017931503872)\n",
      "('BJy_iqr', 0.004580396354579795)\n",
      "('BJz_iqr', 0.00285008172084865)\n",
      "('BJtotal_iqr', 0.015807614088339486)\n",
      "('BJx_skew', 0.0011855909277314366)\n",
      "('BJy_skew', 0.0022999177418860594)\n",
      "('BJz_skew', 0.0009528836811465999)\n",
      "('BJtotal_skew', 0.0009176288855383085)\n",
      "('BJx_entropy', 0.0005830393018549831)\n",
      "('BJy_entropy', 0.0006008243744898216)\n",
      "('BJz_entropy', 0.0006596238788906207)\n",
      "('BJtotal_entropy', 0.024351231132235068)\n",
      "('BJ_sma', 0.016828423321193105)\n",
      "('BJfreq_feature0', 0.0003009721867694756)\n",
      "('BJfreq_feature1', 0.0002681124109918336)\n",
      "('BJfreq_feature2', 0.0002663740647735005)\n",
      "('BJfreq_feature3', 0.0003501381630290611)\n",
      "('BJfreq_feature4', 0.0003181352921378539)\n",
      "('BJfreq_feature5', 0.00023642627489438737)\n",
      "('BJfreq_feature6', 0.00032170062595384964)\n",
      "('BJfreq_feature7', 0.00025940513973296114)\n",
      "('BJfreq_feature8', 0.0002705557803416151)\n",
      "('BJfreq_feature9', 0.00032113960992291243)\n",
      "('BJfreq_feature10', 0.00035174012381360695)\n",
      "('BJfreq_feature11', 0.00022071427408692005)\n",
      "('BJfreq_feature12', 0.0006024183715834047)\n",
      "('BJfreq_feature13', 0.01459354467010751)\n",
      "('Ax_Ay_correleation', 0.001953073881133916)\n",
      "('Ax_Az_correleation', 0.001518339839952818)\n",
      "('Ax_total_Acc_correleation', 0.0027814490777147574)\n",
      "('Ay_Az_correleation', 0.0015363286594308335)\n",
      "('Ay_total_Acc_correleation', 0.0006647138199255077)\n",
      "('Az_total_Acc_correleation', 0.0008789777939598781)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yair\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\base.py:79: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
      "  UserWarning)\n",
      "C:\\Users\\Yair\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\base.py:79: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7668, 0) (7668, 211)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(7668, 0)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-efb6e4a9b0bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;31m# extract_features_main(input_root,output_root)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[0mselection_treshhold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.05\u001b[0m \u001b[1;31m#treshhold for gini ranking for fature selection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m \u001b[0mRandForest_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_root\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mselection_treshhold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-efb6e4a9b0bb>\u001b[0m in \u001b[0;36mRandForest_model\u001b[1;34m(input_path, selection_treshhold)\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;31m## train and estimate Random forest with selected features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[0mrf_important\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m     \u001b[0mrf_important\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportant_data_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m     \u001b[0mpredictions_important\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_important\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimportant_data_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m     \u001b[1;31m## report accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[1;31m# Validate or convert input data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    556\u001b[0m                              \u001b[1;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                              % (n_features, array.shape, ensure_min_features,\n\u001b[1;32m--> 558\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(7668, 0)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy.signal import medfilt\n",
    "from scipy.signal import butter,filtfilt\n",
    "from scipy.signal import iirfilter\n",
    "import itertools\n",
    "from scipy import fftpack\n",
    "from scipy import signal\n",
    "from skimage.util.shape import view_as_windows\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import glob\n",
    "from scipy.fftpack import rfft,fftshift\n",
    "from numpy.fft import fft,fftfreq\n",
    "from scipy.integrate import trapz\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Feature Extraction\n",
    "def load_file(filepath):\n",
    "    column_names = ['Ax','Ay','Az']\n",
    "    dataframe = read_csv(filepath, header=None, names=column_names, delim_whitespace=True)\n",
    "#     get_num_from_path = re.findall(\"\\d\",filepath)\n",
    "    return dataframe\n",
    "\n",
    "#Calculating the entropy - which is the the messure of the randomness of the data\n",
    "def entropy(x):\n",
    "    np_fft = np.fft.fft(x)\n",
    "    amplitudes = 2 / len(x) * np.abs(np_fft)\n",
    "    PSD = np.abs(amplitudes)**2/len(x)\n",
    "    pi = PSD/np.sum(PSD)\n",
    "    e = -np.sum(np.multiply(pi,np.log(pi)))\n",
    "    return(e)\n",
    "            \n",
    "      \n",
    "\n",
    "# IIR filter\n",
    "# low pass filtering (LPF),where custom third-order elliptical infinite impulse\n",
    "# response (IIR) filter with cut-off frequency at 0.25 Hz (0.01 dB passband ripple; stopband at âˆ’100 dB) is employed to\n",
    "# separate the acceleration components due to gravity (GA) and bodily motion (BA) \n",
    "def IIR_lowpass_filter(data, cutoff, fs, rp, rs, order):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    b, a = iirfilter(order, normal_cutoff, rp, rs, btype='low', analog=False, ftype='ellip')\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "#  butter filter\n",
    "# A third order low-pass Butterworth filter with a cutoff frequency of 20 Hz.\n",
    "#since the energy spectrum of the human body motion lies mainly within the range of 0 Hz to 15 Hz\n",
    "def butter_lowpass_filter(data, cutoff,fs_in, order ,fs_out):\n",
    "    nyq = 0.5 * fs_in\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    # Get the filter coefficients \n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False, fs=fs_out)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "## total square root of sum of squers of the data\n",
    "def signalMag(x,y,z):\n",
    "    return np.sqrt(x*x + y*y + z*z)\n",
    "\n",
    "\n",
    "### Calculating frequeancy parameter with DFT - Descrete Fourier Transform\n",
    "## The transform component represent the frequencies with the higher amplitude in the signal\n",
    "\n",
    "def freq_calc(y,index):\n",
    "    n = len(y)\n",
    "    freqs = fftfreq(n)\n",
    "    mask = freqs > 0\n",
    "    fft_val= fft(y,n)\n",
    "    amplitude = 2*np.abs(fft_val/n)\n",
    "    amplitude = amplitude[mask]\n",
    "    max_six_index = np.argpartition(amplitude, -6)[-6:]\n",
    "    max_six_ferq = freqs[mask][max_six_index]\n",
    "    average_freq = np.average(freqs[mask],weights=amplitude)\n",
    "    energy = np.mean(np.sum(amplitude**2))\n",
    "    bandsEnergy = np.sum(amplitude**2)\n",
    "    results = np.hstack([max_six_index, max_six_ferq, average_freq,energy,bandsEnergy])\n",
    "    return results[index]\n",
    "\n",
    "### Compute SMA - normalized signal magnitude area\n",
    "def SMA_calac(header,features, data, window_size):\n",
    "    data_abs = data.abs()\n",
    "    data_inagration = pd.DataFrame()\n",
    "    for col in data.columns:\n",
    "        data_inagration[col] = data_abs[col].rolling(window_size).apply(trapz, raw=False)\n",
    "    features[header+'_sma'] =  (1/2.56)*data_inagration.sum(axis = 1)\n",
    "    return features \n",
    "\n",
    "### Utility function computing statistic features in time and frequency domain \n",
    "def time_freq_domain_feature_calc(header,y,window_size,features):\n",
    "    y['total_Acc']  = signalMag(y['Ax'],y['Ay'],y['Az'])\n",
    "    rolling_window = y.rolling(window_size)\n",
    "    features[[header+'x_mean',header+'y_mean',header+'z_mean',header+'total_mean']] = rolling_window.mean()\n",
    "    features[[header+'x_std',header+'y_std',header+'z_std',header+'total_std']] = rolling_window.std()\n",
    "    features[[header+'x_median',header+'y_median',header+'z_median',header+'total_median']] = rolling_window.median()\n",
    "    features[[header+'x_max',header+'y_max',header+'z_max',header+'total_max']] = rolling_window.max()\n",
    "    features[[header+'x_min',header+'y_min',header+'z_min',header+'total_min']] = rolling_window.min()\n",
    "    features[[header+'x_range',header+'y_range',header+'z_range',header+'total_range']] = rolling_window.max()-rolling_window.min()\n",
    "    features[[header+'x_iqr',header+'y_iqr',header+'z_iqr',header+'total_iqr']] = rolling_window.quantile(0.75)-rolling_window.quantile(0.25)\n",
    "    features[[header+'x_skew',header+'y_skew',header+'z_skew',header+'total_skew']] = rolling_window.skew()\n",
    "    features[[header+'x_entropy',header+'y_entropy',header+'z_entropy',header+'total_entropy']] = rolling_window.apply(entropy, raw=True)\n",
    "    ### Add SMA calac to features\n",
    "    features[header+'_sma'] = SMA_calac(header,features, y[['Ax','Ay','Az']], window_size)\n",
    "    ## calc rolling window for total compnent\n",
    "    rolling_window_total = y['total_Acc'].rolling(window_size)\n",
    "       \n",
    "    #Calc freq features on the total component\n",
    "    for i in np.arange(14):\n",
    "        fet_temp = rolling_window_total.apply(lambda x: freq_calc(x,i))\n",
    "        features[header+'freq_feature'+str(i)]=fet_temp[::]\n",
    "        \n",
    "        \n",
    "    return features\n",
    "\n",
    "# calculation the jerk - the derivative of the acceleration by (dA/dt)\n",
    "def jerk_calc(df,dt):\n",
    "    jerk_db=df.diff()/dt\n",
    "    return jerk_db\n",
    "\n",
    "#calculation cross correlation between Axes\n",
    "def cross_correlation(features, data, window_size):\n",
    "    corr = data.rolling(window_size).corr(pairwise=True)\n",
    "    combo = list(itertools.combinations(data.columns,2))\n",
    "    for i in np.arange(len(combo)):\n",
    "        features[combo[i][0]+'_'+combo[i][1]+'_correleation']= corr[combo[i][0]].xs(combo[i][1],level=1)\n",
    "    return features\n",
    "        \n",
    "\n",
    "\n",
    "## Main function for signal processing and feature extraction\n",
    "def featuer_extraction(input_root,file_name, start, end, label):\n",
    "    data = load_file(f\"{input_root}/{file_name}\")\n",
    "    #Cut relevant part using end,start\n",
    "    data = data.truncate(before=start-1, after=end-1)\n",
    "    ### Apply Median filter with window=5 to reduce white noise (normal noise)\n",
    "    TA_med_filter= pd.DataFrame({'Ax':medfilt(data.Ax,kernel_size= 3),\n",
    "                                   'Ay':medfilt(data.Ay,kernel_size=3),\n",
    "                                   'Az':medfilt(data.Az,kernel_size=3)})\n",
    "    \n",
    "    #plot_acc_together(data, TA_med_filter, 'Compare TA raw and with median Filter')\n",
    "    ### A third order low-pass Butterworth filter with a cutoff frequency of 20 Hz.\n",
    "    fs_in = 50       # sample rate of source data [Hz]\n",
    "    cutoff = 20 # The Low-pass filter, passes signals with a frequency lower than a certain cutoff frequency and attenuates signals with frequencies higher than the cutoff frequency.\n",
    "    order=3\n",
    "    fs_out = 20\n",
    "    TA_lowpass_filter = pd.DataFrame({'Ax':butter_lowpass_filter(TA_med_filter.Ax, cutoff, fs_in, order,fs_out),\n",
    "                                   'Ay':butter_lowpass_filter(TA_med_filter.Ay, cutoff, fs_in, order,fs_out),\n",
    "                                   'Az':butter_lowpass_filter(TA_med_filter.Az, cutoff, fs_in, order,fs_out)})\n",
    "    #plot_acc_together(TA_med_filter, TA_lowpass_filter, 'Compare TA with median Filter and low pass filter')\n",
    "    #### Gravity calculation\n",
    "    fs= 50       # sample rate of source data [Hz]\n",
    "    cutoff_gravity = 0.25 # The Low-pass filter, passes signals with a frequency lower than a \n",
    "                            #certain cutoff frequency and attenuates signals with frequencies higher than the cutoff frequency.\n",
    "    order=3\n",
    "    rp = 0.01 #passband ripple\n",
    "    rs = 100 # stop band \n",
    "    #The Gravity Acceleration (GA) component is taken directly from the result of applying the LPF to the TA_lowpass_filter \n",
    "    Gravity = pd.DataFrame({'Ax':IIR_lowpass_filter(TA_lowpass_filter.Ax, cutoff_gravity, fs, rp, rs, order),\n",
    "                               'Ay':IIR_lowpass_filter(TA_lowpass_filter.Ay, cutoff_gravity, fs, rp, rs, order),\n",
    "                               'Az':IIR_lowpass_filter(TA_lowpass_filter.Az, cutoff_gravity, fs, rp, rs, order)})\n",
    "    # Body Acceleraton (BA) component is taken as the difference between the original signal and the GA component\n",
    "    \n",
    "    BA_data= pd.DataFrame({'Ax':TA_lowpass_filter.Ax-Gravity.Ax,\n",
    "                               'Ay':TA_lowpass_filter.Ay-Gravity.Ay,\n",
    "                               'Az':TA_lowpass_filter.Az-Gravity.Az})\n",
    "    \n",
    "    #plot_acc_together(data_lowpass_filter, body_acceleration, 'Compare TA with BA filter')\n",
    "    ## calculate jerk from BA\n",
    "    dt = 1/50\n",
    "    jerk_BA = jerk_calc(BA_data,dt)\n",
    "    # Sloding window - each with a span of 2.56 sec and an overlap of 50% . This was determined according to \n",
    "    # values in literature, and is known to cpature human movment. 128 sample = 2.56 sec * 5-Htz sampling rate\n",
    "    # overlaping - 64 samles\n",
    "    window_size = 128\n",
    "    features = pd.DataFrame()\n",
    "    # TA = Total acceleration\n",
    "    features = time_freq_domain_feature_calc('TA',TA_lowpass_filter,window_size,features)\n",
    "    # BA = body acceleration\n",
    "    features = time_freq_domain_feature_calc('BA',BA_data,window_size,features)\n",
    "    # GA = Gravity acceleration\n",
    "    features = time_freq_domain_feature_calc('GA',Gravity,window_size,features)\n",
    "    # BJ = Body Jerk\n",
    "    features = time_freq_domain_feature_calc('BJ',jerk_BA,window_size,features)  \n",
    "    ### Compute cross correlation\n",
    "    features = cross_correlation(features, BA_data, window_size)\n",
    "    ### selecting only sliding window with 50% overlapping\n",
    "    features = features[128::64]\n",
    "    ### setting the labling of this set\n",
    "    features['label']= np.ones(len(features))*label\n",
    "    return features \n",
    "\n",
    "\n",
    "\n",
    "#### fumction that uploads the HARP dataset from input_root in local machone and creates train and test data\n",
    "## Train and test files are saved to output root\n",
    "## train, test - initialized dataframes. \n",
    "## partitian is made with 70% train in 30% test\n",
    "#ind_test, ind_train - in case of partial run, can be used to save running time.\n",
    "def load_dataset(input_root,train,test,ind_train,ind_test):\n",
    "    files_list = glob.glob(input_root+'acc*.*')\n",
    "    column_names = ['exp_id','user_id','act_id','label_start','label_end']\n",
    "    labels = read_csv(input_root+'labels.txt', header=None, names=column_names, delim_whitespace=True)\n",
    "    X = labels[['exp_id','user_id','label_start','label_end']] \n",
    "    Y = labels['act_id']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "    #create_train_set\n",
    "    X_train = X_train.reset_index()\n",
    "    y_train = y_train.reset_index()\n",
    "    train = pd.DataFrame()\n",
    "    for i in np.arange(ind_train,len(X_train)):\n",
    "        #Validatin lenght of the segment so i will not be less then 2.56sec.\n",
    "        #Otherwise the segment is excluded from the data\n",
    "        if X_train.label_end[i]-X_train.label_start[i]>=129: \n",
    "            file_name='acc_exp{:0>2d}_user{:0>2d}.txt'.format(X_train.exp_id[i],X_train.user_id[i],y_train.act_id[i])\n",
    "            train = train.append(\n",
    "                featuer_extraction(input_root,file_name, X_train.label_start[i], X_train.label_end[i],y_train.act_id[i]),\n",
    "                ignore_index=True)\n",
    "        else:\n",
    "            print('sample_'+str(i)+'was excluded')\n",
    "    print('finish_train')\n",
    "\n",
    "    #create_test_set\n",
    "    X_test = X_test.reset_index()\n",
    "    y_test = y_test.reset_index()\n",
    "    test = pd.DataFrame()\n",
    "    for i in np.arange(ind_test,len(X_test)):\n",
    "        #Validatin lenght of the segment so i will not be less then 2.56sec.\n",
    "        #Otherwise the segment is excluded from the data\n",
    "        if X_test.label_end[i]-X_test.label_start[i]>=129:\n",
    "            file_name='acc_exp{:0>2d}_user{:0>2d}.txt'.format(X_test.exp_id[i],X_test.user_id[i],y_test.act_id[i])\n",
    "            test = test.append(\n",
    "                featuer_extraction(input_root,file_name, X_test.label_start[i], X_test.label_end[i],y_test.act_id[i]),\n",
    "                ignore_index=True)\n",
    "               \n",
    "        else:\n",
    "            print('sample_'+str(i)+'was excluded')\n",
    "    print('finish_test')\n",
    "    return train, test\n",
    "\n",
    "\n",
    "## utility fimction to activate load_dataset function \n",
    "def extract_features_main(input_root,output_root):\n",
    "    # insert ind_train = 780 and ind_test = 290 for a short run of the feature extraction module\n",
    "    ind_train = 0\n",
    "    ind_test = 0\n",
    "    test = pd.DataFrame()\n",
    "    train = pd.DataFrame()\n",
    "    train,test = load_dataset(input_root,train,test,ind_train,ind_test)\n",
    "    train.to_csv(output_root+'train_new.csv')\n",
    "    test.to_csv(output_root+'test_new.csv')\n",
    "    \n",
    "\n",
    "## Main function for creating and training random forest model withe feature ranking and selection\n",
    "def RandForest_model(input_path,selection_treshhold):\n",
    "    ## excecute model with data saved in output folder\n",
    "    ### load Train and split features and labels\n",
    "    data_train = pd.read_csv( input_path+\"train_new.csv\")\n",
    "    labels_train = data_train.label\n",
    "    data_train=data_train.drop('label', axis=1)\n",
    "    ### load Train and split features and labels\n",
    "    data_test = pd.read_csv(input_path+\"test_new.csv\")\n",
    "    labels_test = data_test.label\n",
    "    data_test = data_test.drop('label', axis=1)\n",
    "    print( \"loading done\")\n",
    "\n",
    "    # Create the model with 1000 trees\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=1000, \n",
    "                                   bootstrap = True,\n",
    "                                   max_features = 'sqrt',\n",
    "                                  criterion= 'entropy')\n",
    "\n",
    "    rf = RandomForestClassifier(n_estimators=1000,random_state=0, n_jobs=-1)\n",
    "    rf.fit(data_train, labels_train)\n",
    "    predictions = rf.predict(data_test)\n",
    "    ## report accuracy\n",
    "    print('accuracy:' + str(np.sum(predictions == labels_test)/predictions.shape[0]))\n",
    "    print(classification_report(labels_test, predictions, digits = 4))\n",
    "    \n",
    "    #Feture  scoring and selection\n",
    "    feat_labels = data_train.columns\n",
    "    for feature in zip(feat_labels, rf.feature_importances_):\n",
    "        print(feature)\n",
    "    \n",
    "    ###select the features above gini score as determined in selection_treshhold\n",
    "    selcted_features = SelectFromModel(rf, prefit=True, threshold=selection_treshhold)\n",
    "    \n",
    "    # Print the names of the most important features\n",
    "    for feature_list_index in selcted_features.get_support(indices=True):\n",
    "        print(feat_labels[feature_list_index])\n",
    "    \n",
    "    important_data_train = selcted_features.transform(data_train)\n",
    "    important_data_test = selcted_features.transform(data_test)\n",
    "    \n",
    "    # Print the shape of the train dataframe before and after feature selection to varify it os changed\n",
    "    print(important_data_train.shape, data_train.shape)\n",
    "    \n",
    "    ## train and estimate Random forest with selected features\n",
    "    rf_important = RandomForestClassifier(n_estimators=1000, random_state=0, n_jobs=-1)\n",
    "    rf_important.fit(important_data_train, labels_train)\n",
    "    predictions_important = rf_important.predict(important_data_test)\n",
    "    ## report accuracy\n",
    "    print('accuracy:' + str(np.sum(predictions_important == labels_test)/predictions_important.shape[0]))\n",
    "    print(classification_report(labels_test, predictions_important, digits = 4))\n",
    " \n",
    "#This code allow control on executing the model. \n",
    "#inpur root - where HARP database is saved in folder\n",
    "input_root = r'C:/Users/Yair/Documents/Airtasker/Akhila/HAPT Data Set/RawData/'\n",
    "#Output root - where to store fetures . using the same path to lunch them for in RandForest_model \n",
    "output_root = r'C:\\Users\\Yair\\Documents\\Airtasker\\Akhila\\HAPT Data Set\\Extracted_Features\\\\'\n",
    "### Deactivate this line to avoid feature extracture\n",
    "extract_features_main(input_root,output_root)\n",
    "selection_treshhold = 0.05 #treshhold for gini ranking for fature selection\n",
    "RandForest_model(output_root,selection_treshhold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-5e6a15ce28a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "train.shapeddd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
